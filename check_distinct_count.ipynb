{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==2.4.3 in /opt/app-root/lib/python3.6/site-packages (2.4.3)\n",
      "Requirement already satisfied: py4j==0.10.7 in /opt/app-root/lib/python3.6/site-packages (from pyspark==2.4.3) (0.10.7)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 20.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==2.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pyspark\n",
    "import pandas\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import types as t\n",
    "import socket\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source_buckets import source_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Cluster: spark://172.44.45.6:7077\n",
      "S3 endpoint: https://s3.upshift.redhat.com/\n",
      "Spark App Name: repartition-multiple-buckets-2020-08-19 11:38\n",
      "Hostname: 172.44.44.242\n"
     ]
    }
   ],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = '/opt/app-root/bin/python3'\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = '/opt/app-root/bin/python3'\n",
    "# spark.jars.ivy={os.environ['HOME']}\n",
    "SPARK_CLUSTER = 'spark://172.44.45.6:7077'\n",
    "S3_ENDPOINT = 'https://s3.upshift.redhat.com/'\n",
    "SPARK_APP_NAME = f'repartition-multiple-buckets-{datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")}'\n",
    "HOSTNAME = socket.gethostbyname(socket.gethostname())\n",
    "print('Spark Cluster: {}'.format(SPARK_CLUSTER))\n",
    "print('S3 endpoint: {}'.format(S3_ENDPOINT))\n",
    "print('Spark App Name: {}'.format(SPARK_APP_NAME))\n",
    "print('Hostname: {}'.format(HOSTNAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_config(spark_cluster, executor_memory='16g', executor_cores='4', max_cores='16'):\n",
    "    print('Spark cluster is: {}'.format(spark_cluster))\n",
    "    sc_conf = (\n",
    "        pyspark.SparkConf().setMaster(spark_cluster) \\\n",
    "        .set('spark.driver.host', HOSTNAME) \\\n",
    "        .set('spark.driver.port', 42000) \\\n",
    "        .set('spark.driver.bindAddress', '0.0.0.0') \\\n",
    "        .set('spark.driver.blockManager.port', 42100) \\\n",
    "        .set('spark.executor.cores', '3') \\\n",
    "        .set('spark.executor.memory', '4500M') \\\n",
    "        .set('spark.driver.memory', '4G') \\\n",
    "        .set('spark.sql.parquet.enableVectorizedReader', True) \\\n",
    "        .set('spark.kubernetes.memoryOverheadFactor', '0.20')\n",
    "    )\n",
    "    return sc_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_spark():\n",
    "    spark_config = create_spark_config(SPARK_CLUSTER)\n",
    "    print('spark_config is: {}'.format(spark_config))\n",
    "    print(\"Creating Spark Session at cluster: {}\".format(SPARK_CLUSTER))\n",
    "    spark = SparkSession.builder.appName(SPARK_APP_NAME).enableHiveSupport().config(conf=spark_config).getOrCreate()\n",
    "    spark.sparkContext.setLogLevel('ERROR')\n",
    "    hadoopConf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "    hadoopConf.set('fs.s3a.endpoint', S3_ENDPOINT)\n",
    "    hadoopConf.set('fs.s3a.path.style.access', 'true')\n",
    "    hadoopConf.set('fs.s3a.access.key', os.environ.get('AWS_ACCESS_KEY_ID'))\n",
    "    hadoopConf.set('fs.s3a.secret.key', os.environ.get('AWS_SECRET_ACCESS_KEY'))\n",
    "    hadoopConf.set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "    print(\"hadoop is configured!\")\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_path_string(bucket_name, repartitioned_data=False):\n",
    "    table_name = bucket_name[3:-4].lower().replace('-', '_')\n",
    "#     print('table_name: {}'.format(table_name))\n",
    "    if not repartitioned_data:\n",
    "        path_string = 's3a://{}/extraction/sos/parquet/{}/'.format(bucket_name, table_name)\n",
    "    else:\n",
    "        path_string = 's3a://{}/extraction/sos/parquet/{}/'.format('DH-SECURE-SOSREPORTS', table_name)\n",
    "    return path_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe_from_bucket(bucket_name, repartitioned_data=False):\n",
    "    src_path = form_path_string(bucket_name, repartitioned_data)\n",
    "#     print(src_path)\n",
    "    df = spark.read.parquet(f'{src_path}')\n",
    "    count = df.count()\n",
    "    distinct_count = df.distinct().count()\n",
    "    num_partitions = df.rdd.getNumPartitions()\n",
    "    return (count, distinct_count, num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark cluster is: spark://172.44.45.6:7077\n",
      "spark_config is: <pyspark.conf.SparkConf object at 0x7fdf3643e0b8>\n",
      "Creating Spark Session at cluster: spark://172.44.45.6:7077\n",
      "hadoop is configured!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.stop()\n",
    "    spark = setup_spark()\n",
    "except:\n",
    "    spark = setup_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_buckets = [\n",
    "    'DH-KERBEROS-KDC-LOG-TMP',\n",
    "    'DH-KEYSTONE-LOG-TMP',\n",
    "    'DH-LSOF-TMP',\n",
    "    'DH-MESSAGES-TMP',\n",
    "    'DH-MULTIPATH--V4--LL-TMP',\n",
    "    'DH-MULTIPATH-LL-TMP',\n",
    "    'DH-NETSTAT-TMP',\n",
    "    'DH-NEUTRON-L3-AGENT-LOG-TMP',\n",
    "    'DH-NEUTRON-OVS-AGENT-LOG-TMP',\n",
    "    'DH-NEUTRON-SERVER-TMP',\n",
    "    'DH-NFS-EXPORTS-TMP',\n",
    "    'DH-NOVA-API-LOG-TMP',\n",
    "    'DH-NOVA-COMPUTE-LOG-TMP',\n",
    "    'DH-OPENSTACK-ROUTER-LIST-TMP',\n",
    "    'DH-OPENSTACK-SECURITY-GROUP-LIST-TMP',\n",
    "    'DH-OSA-DISPATCHER-LOG-TMP',\n",
    "    'DH-POSTGRESQL-LOG-TMP',\n",
    "    'DH-PS-AUX-TMP',\n",
    "    'DH-PS-AUXWW-TMP',\n",
    "    'DH-RABBITMQ-REPORT-TMP',\n",
    "    'DH-RHSM-LOG-TMP',\n",
    "    'DH-ROUTE-TMP',\n",
    "    'DH-SAMBA-TMP',\n",
    "    'DH-SCSI-TMP',\n",
    "    'DH-SECURE-TMP',\n",
    "    'DH-SIMPLE-FILE-TMP',\n",
    "    'DH-SS-TMP',\n",
    "    'DH-SYSCONFIG-KDUMP-TMP',\n",
    "    'DH-SYSCONFIG-VIRT-WHO-TMP',\n",
    "    'DH-SYSCTL-TMP',\n",
    "    'DH-UP2DATE-TMP',\n",
    "    'DH-VDSM-LOG-TMP',\n",
    "    'DH-VGDISPLAY-TMP',\n",
    "    'DH-VMCORE-DMESG-TMP',\n",
    "    'DH-VSFTPD-TMP',\n",
    "    'DH-YUM-REPOS-D-TMP',\n",
    "    'DH-JOURNAL-SINCE-BOOT-TMP',\n",
    "    'DH-KDUMP-TMP'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number: 233, Bucket: DH-XINETD-CONF-TMP\n",
      "Partitions reduced by: 603\n",
      "Number: 234, Bucket: DH-YUM-CONF-TMP\n",
      "Partitions reduced by: 636\n",
      "Number: 235, Bucket: DH-YUM-LOG-TMP\n",
      "Partitions reduced by: 616\n",
      "Number: 236, Bucket: DH-YUM-REPOLIST-TMP\n",
      "Partitions reduced by: 636\n",
      "Number: 237, Bucket: DH-YUM-REPOS-D-TMP\n",
      "Not checking a failed bucket.\n",
      "Number: 238, Bucket: DH-ZIPL-CONF-TMP\n",
      "Partitions reduced by: 0\n",
      "Number: 239, Bucket: DH-JOURNAL-SINCE-BOOT-TMP\n",
      "Not checking a failed bucket.\n",
      "Number: 240, Bucket: DH-KDUMP-TMP\n",
      "Not checking a failed bucket.\n"
     ]
    }
   ],
   "source": [
    "for count, bucket_name in enumerate(source_buckets):\n",
    "    if count < 232: continue\n",
    "    print('Number: {}, Bucket: {}'.format(count+1, bucket_name))\n",
    "    if bucket_name in failed_buckets:\n",
    "        print('Not checking a failed bucket.')\n",
    "        continue\n",
    "    initial_count, intial_distinct_count, initial_partitions_count = read_dataframe_from_bucket(bucket_name)\n",
    "    final_count, final_distinct_count, final_partitions_count = read_dataframe_from_bucket(bucket_name, True)\n",
    "    if intial_distinct_count != final_distinct_count:\n",
    "        print('Data Loss:')\n",
    "        print(bucket_name, intial_distinct_count, final_distinct_count)\n",
    "    print('Partitions reduced by: {}'.format(initial_partitions_count- final_partitions_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
